{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivakrishnaah/advanced_deep_learning/blob/main/PixelCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-AMvIot47Ph"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RnL0IwX306K"
      },
      "outputs": [],
      "source": [
        "#! /usr/bin/env python\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim, cuda, backends\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms, utils\n",
        "backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmqu52uS5xTu"
      },
      "source": [
        "## [Pixel CNN paper ](https://arxiv.org/abs/1606.05328)\n",
        "### Mask Type A\n",
        "\t- Used in the first convolutional layer.\n",
        "\t- Ensures that the current pixel cannot see itself during prediction.\n",
        "\t- The mask removes the center pixel and all future pixels in both row and column directions.\n",
        "\n",
        "### Mask Type B\n",
        "\t- Used in subsequent convolutional layers.\n",
        "\t- Allows the network to see the current pixel but not future pixels.\n",
        "\t- The mask removes only future pixels, while the current pixel remains accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KLn9GYZ5AoH"
      },
      "outputs": [],
      "source": [
        "class MaskedConv2d(nn.Conv2d):\n",
        "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n",
        "        super(MaskedConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
        "        assert mask_type in {'A', 'B'}\n",
        "        self.register_buffer('mask', self.weight.data.clone())\n",
        "        _, _, height, width = self.weight.size()\n",
        "        self.mask.fill_(1)\n",
        "        yc, xc = height // 2, width // 2\n",
        "        self.mask[:, :, yc, xc+ (mask_type == 'B'):] = 0\n",
        "        self.mask[:, :, yc + 1:] = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.weight.data *= self.mask\n",
        "        return super(MaskedConv2d, self).forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RpEgm575Fvl"
      },
      "outputs": [],
      "source": [
        "class PixelCNN(nn.Module):\n",
        "  def __init__(self, input_channels=3, n_filters=64, kernel_size=7, n_layers=8) -> None:\n",
        "      super(PixelCNN, self).__init__()\n",
        "      layers = []\n",
        "      layers.append(MaskedConv2d('A', input_channels, n_filters, kernel_size, 1, padding=kernel_size//2, bias=False))\n",
        "      layers.append(nn.BatchNorm2d(n_filters))\n",
        "      layers.append(nn.ReLU(True))\n",
        "\n",
        "      for i in range(n_layers -1):\n",
        "          layers.append(MaskedConv2d('B', n_filters, n_filters, kernel_size, 1, padding=kernel_size//2, bias=False))\n",
        "          layers.append(nn.BatchNorm2d(n_filters))\n",
        "          layers.append(nn.ReLU(True))\n",
        "          # nn.Conv2d(in_channels=input_channels,  out_channels=input_channels*256, kernel_size=7, padding=3)\n",
        "      layers.append(nn.Conv2d(in_channels=n_filters, out_channels=input_channels * 256, kernel_size=1))\n",
        "      self.net = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxJHM9OO5J9D"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def training_model(model, train_loader, optimizer, device, epochs=10):\n",
        "    # Detect the channels from the input\n",
        "    input_channels = next(iter(train_loader))[0].shape[1]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, _ in train_loader:\n",
        "            # Scale target to integers in [0, 255]\n",
        "            x = (x * 255).long().to(device)\n",
        "            x_input = x.float() / 255.0\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(x_input)          # [batch, input_channels * 256, H, W]\n",
        "            batch_size, _, height, width = logits.shape\n",
        "\n",
        "            # Reshape logits: [batch, channels, 256, H, W]\n",
        "            logits = logits.view(batch_size, input_channels, 256, height, width)\n",
        "\n",
        "            # Move channels to last dimension: [batch, H, W, channels, 256]\n",
        "            logits = logits.permute(0, 3, 4, 1, 2).contiguous()\n",
        "\n",
        "            # Flatten for loss: [batch * H * W * channels, 256]\n",
        "            logits = logits.view(-1, 256)\n",
        "            targets = x.view(-1)  # Flatten targets: [batch * H * W * channels]\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGmO0ewe5MsK",
        "outputId": "e3b5d5a2-5410-4ab5-bc04-1c9907f15d45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.11MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.47MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 1.3173\n",
            "Epoch 2: Loss = 0.7900\n",
            "Epoch 3: Loss = 0.7625\n",
            "Epoch 4: Loss = 0.7494\n",
            "Epoch 5: Loss = 0.7405\n",
            "Epoch 6: Loss = 0.7334\n",
            "Epoch 7: Loss = 0.7272\n",
            "Epoch 8: Loss = 0.7213\n",
            "Epoch 9: Loss = 0.7147\n",
            "Epoch 10: Loss = 0.7073\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_loader = data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
        "    batch_size=128, shuffle=True\n",
        ")\n",
        "sample_batch, _ = next(iter(train_loader))\n",
        "input_channels = sample_batch.shape[1]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "minst_model = PixelCNN(input_channels=input_channels, n_filters=64).to(device)\n",
        "optimizer = torch.optim.Adam(minst_model.parameters(), lr=1e-3)\n",
        "training_model(minst_model, train_loader, optimizer, device, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuMBtwwf5Pw2"
      },
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "def sample(model, img_size=(28, 28), n_channels=1, n_samples=64):\n",
        "    model.eval()\n",
        "    samples = torch.zeros(n_samples, n_channels, *img_size).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(img_size[0]):\n",
        "            for j in range(img_size[1]):\n",
        "              for c in range(n_channels):\n",
        "                logits = model(samples)\n",
        "                logits = logits.view(n_samples, n_channels, 256, img_size[0], img_size[1])\n",
        "                probs = F.softmax(logits[:, c, :, i, j], dim=1)  # [n_samples, 256]\n",
        "                pixel_values = torch.multinomial(probs, num_samples=1).squeeze(-1)  # Sample pixel\n",
        "                samples[:, c, i, j] = pixel_values.float() / 255.0\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0V_8LLT5R3w"
      },
      "outputs": [],
      "source": [
        "samples = sample(minst_model, img_size=(28, 28), n_channels=1, n_samples=64)\n",
        "vutils.save_image(samples, \"pixelcnn_mnist_samples1.png\", nrow=8, padding=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKnoUMBN5Utk",
        "outputId": "985448ca-a179-42b0-972e-8642071597a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 5.1866\n",
            "Epoch 2: Loss = 4.8837\n",
            "Epoch 3: Loss = 4.7629\n",
            "Epoch 4: Loss = 4.6849\n",
            "Epoch 5: Loss = 4.6198\n",
            "Epoch 6: Loss = 4.5785\n",
            "Epoch 7: Loss = 4.5464\n",
            "Epoch 8: Loss = 4.5156\n",
            "Epoch 9: Loss = 4.4803\n",
            "Epoch 10: Loss = 4.4465\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_loader = data.DataLoader(\n",
        "    datasets.CIFAR10('data', train=True, download=True, transform=transform),\n",
        "    batch_size=128, shuffle=True\n",
        ")\n",
        "sample_batch, _ = next(iter(train_loader))\n",
        "input_channels = sample_batch.shape[1]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cifar_model = PixelCNN(input_channels=input_channels, n_filters=192, n_layers=15).to(device)\n",
        "optimizer = torch.optim.Adam(cifar_model.parameters(), lr=1e-3)\n",
        "training_model(cifar_model, train_loader, optimizer, device, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SJ_KSjXt5XQr"
      },
      "outputs": [],
      "source": [
        "samples = sample(cifar_model, img_size=(32, 32), n_channels=3, n_samples=64)\n",
        "vutils.save_image(samples, \"pixelcnn_cifar10_samples.png\", nrow=8, padding=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j0R3yuVJ9EHy"
      },
      "outputs": [],
      "source": [
        "class GatedMaskedConv2d(nn.Module):\n",
        "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(GatedMaskedConv2d, self).__init__()\n",
        "        self.mask_type = mask_type\n",
        "        self.conv = MaskedConv2d(mask_type, in_channels, 2 * out_channels, kernel_size, stride, padding)\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        value, gate = out.chunk(2, dim=1)\n",
        "        return torch.tanh(value) * torch.sigmoid(gate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1yllQfU19Qbq"
      },
      "outputs": [],
      "source": [
        "class PixelCNNpp(nn.Module):\n",
        "    def __init__(self, input_channels=3, n_filters=64, kernel_size=7, n_layers=8):\n",
        "        super(PixelCNNpp, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        # First layer uses Mask-A\n",
        "        layers.append(GatedMaskedConv2d('A', input_channels, n_filters, kernel_size, padding=kernel_size//2))\n",
        "\n",
        "        # Subsequent layers use Mask-B\n",
        "        for _ in range(n_layers - 1):\n",
        "            layers.append(GatedMaskedConv2d('B', n_filters, n_filters, kernel_size, padding=kernel_size//2))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.output_conv = nn.Conv2d(n_filters, input_channels * 256, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = self.output_conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG4chDs39ZsE"
      },
      "outputs": [],
      "source": [
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor()\n",
        "# ])\n",
        "# train_loader = data.DataLoader(\n",
        "#     datasets.CIFAR10('data', train=True, download=True, transform=transform),\n",
        "#     batch_size=128, shuffle=True\n",
        "# )\n",
        "# sample_batch, _ = next(iter(train_loader))\n",
        "# input_channels = sample_batch.shape[1]\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cifar_model_pp = PixelCNNpp(input_channels=input_channels, n_filters=192, n_layers=15).to(device)\n",
        "optimizer = torch.optim.Adam(cifar_model_pp.parameters(), lr=1e-3)\n",
        "training_model(cifar_model_pp, train_loader, optimizer, device, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0i5wFn89oHT"
      },
      "outputs": [],
      "source": [
        "samples = sample(cifar_model_pp, img_size=(32, 32), n_channels=3, n_samples=64)\n",
        "vutils.save_image(samples, \"pixelcnn_cifar10pp_samples.png\", nrow=8, padding=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyMo2ShpN9L38ioaugvOhsqO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}